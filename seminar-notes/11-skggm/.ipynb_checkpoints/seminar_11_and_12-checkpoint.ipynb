{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Семинар 11. Адаптивный дизайн эксперимента и суррогатное моделирование\n",
    "\n",
    "## План семинара:\n",
    "\n",
    "  * 0. Виды дизайна эксперимента. Факториальные дизайны.\n",
    "  * 1. Методы заполнения для рандомизированных дизайнов эксперимента;\n",
    "  * 2. Работа в библиотеке Surrogate Optimization Toolbox (pySOT) `pySOT`:\n",
    "  https://pysot.readthedocs.io/en/latest/tutorials.html#tutorial-4-python-objective-function-with-inequality-constraints\n",
    "  * 3*.Визуализация семплирования на 3-х мерных функциях;\n",
    "  * 4. skggm -- графическое лассо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделяют три вида дизайнов эксперимента:\n",
    "    \n",
    "* Screening (Factorial) - Для выявления наиболее важных факторов ( нализ категориальных признаков).\n",
    "* Response Surface - Для анализа сигналов или процессов (напр. моделирование временных процессов)\n",
    "* Mixture - Смешанный случай (напр. лекарственная формация)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Факториальный дизайн эксперимента"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "! pip install dexpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Рассмотрим 2 простых примера:\n",
    "1. Оптимально сваренный кофе: \n",
    "    \n",
    "   Пример взят из: https://www.statease.com/publications/newsletter/stat-teaser-09-16#article1\n",
    "    \n",
    "   Мы хотим понять вклад каждого фактора и уменьшить количество экспериментов (c $2^5=32$ до $24$): \n",
    "   \n",
    "   * Amount of Coffee (2.5 to 4.0 oz.)\n",
    "   * Grind size (8-10mm)\n",
    "   * Brew time (3.5 to 4.5 minutes)\n",
    "   * Grind Type (burr vs blade)\n",
    "   * Coffee beans (light vs dark)\n",
    "\n",
    "Итоговая оценка дается в баллах(1-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dexpy.factorial\n",
    "import dexpy.power\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import patsy\n",
    "import statsmodels.formula.api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# строим матрицу дизайна эксперимента по факторам\n",
    "coffee_design = dexpy.factorial.build_factorial(5, 2**(5))\n",
    "coffee_design.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исследовалась полная факториальная матрица, то есть 32 комбинации. \n",
    "К которым добавили 4 прохода по центральным значениям, для повешения точности прогноза. \n",
    "\n",
    "http://www.stat.purdue.edu/~zhanghao/STAT514/handout/chapter03/PowerSampleSize.pdf\n",
    "\n",
    "https://stats.stackexchange.com/questions/80048/calculating-power-function-for-anova\n",
    "\n",
    "https://ru.wikipedia.org/wiki/%D0%A1%D1%82%D0%B0%D1%82%D0%B8%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D0%BC%D0%BE%D1%89%D0%BD%D0%BE%D1%81%D1%82%D1%8C\n",
    "\n",
    "https://en.wikipedia.org/wiki/F-distribution\n",
    "\n",
    "https://en.wikipedia.org/wiki/F-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['amount', 'grind_size', 'brew_time', 'grind_type', 'beans']\n",
    "coffee_design.columns = column_names\n",
    "center_points = [\n",
    "    [0, 0, 0, -1, -1],\n",
    "    [0, 0, 0, -1,  1],\n",
    "    [0, 0, 0,  1, -1],\n",
    "    [0, 0, 0,  1,  1]\n",
    "]\n",
    "\n",
    "cp_df = pd.DataFrame(center_points * 2, columns=coffee_design.columns)\n",
    "coffee_design = coffee_design.append(cp_df)\n",
    "coffee_design.index = np.arange(0, len(coffee_design))\n",
    "\n",
    "sn = 2.0\n",
    "alpha = 0.1\n",
    "model = ' + '.join(coffee_design.columns)\n",
    "print('Модель:{}'.format(model))\n",
    "factorial_power = dexpy.power.f_power(model, coffee_design, sn, alpha)\n",
    "factorial_power.pop(0) \n",
    "\n",
    "# convert to %\n",
    "factorial_power = ['{0:.2f}%'.format(i*100) for i in factorial_power]\n",
    "factorial_power = pd.DataFrame(factorial_power,\n",
    "                               columns=['Power'],\n",
    "                               index=coffee_design.columns)\n",
    "print(\"\\nPower for full factorial:\")\n",
    "print(factorial_power)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что будет, если мы уменьшим количество комбинаций до $ 2^4=16$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# а теперь посчитаем для \n",
    "coffee_design = dexpy.factorial.build_factorial(5, 2**(5-1))\n",
    "coffee_design.columns = column_names\n",
    "center_points = [\n",
    "    [0, 0, 0, -1, -1],\n",
    "    [0, 0, 0, -1, 1],\n",
    "    [0, 0, 0, 1, -1],\n",
    "    [0, 0, 0, 1, 1]\n",
    "]\n",
    "\n",
    "coffee_design = coffee_design.append(pd.DataFrame(center_points * 2, columns=coffee_design.columns))\n",
    "coffee_design.index = np.arange(0, len(coffee_design))\n",
    "\n",
    "model = ' + '.join(coffee_design.columns)\n",
    "factorial_power = dexpy.power.f_power(model, coffee_design, sn, alpha)\n",
    "factorial_power.pop(0) \n",
    "factorial_power = ['{0:.2f}%'.format(i*100) for i in factorial_power] # перевод в %\n",
    "factorial_power = pd.DataFrame(factorial_power,\n",
    "                               columns=['Power'],\n",
    "                               index=coffee_design.columns)\n",
    "\n",
    "print(\"\\nPower for fractional factorial:\")\n",
    "print(factorial_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_design.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twofi_model = \"(\" + '+'.join(coffee_design.columns) + \")**2\"\n",
    "desc = patsy.ModelDesc.from_formula(twofi_model)\n",
    "factorial_power = dexpy.power.f_power(twofi_model, coffee_design, sn, alpha)\n",
    "factorial_power.pop(0) # remove intercept\n",
    "factorial_power = ['{0:.2f}%'.format(i*100) for i in factorial_power] # convert to %\n",
    "factorial_power = pd.DataFrame(factorial_power,\n",
    "                               columns=['Power'],\n",
    "                               index=desc.describe().strip(\"~ \").split(\" + \"))\n",
    "\n",
    "print(\"\\nPower for fractional factorial (2FI model):\")\n",
    "print(factorial_power)\n",
    "\n",
    "# захардкоженные результаты эксперимента\n",
    "coffee_design['taste_rating'] = [\n",
    "    4.4, 5.8, 6.8, 4.6, 2.6, 5, 6.2, 3.4,\n",
    "    5.6, 5, 5.8, 6.6, 6.2, 3.4, 5, 6.4,\n",
    "    6, 6, 6.8, 6, 6, 6.2, 5, 6.4\n",
    "]\n",
    "\n",
    "# Построим регрессиионный анализ МНК на данных\n",
    "lm = statsmodels.formula.api.ols(\"taste_rating ~\" + twofi_model, data=coffee_design).fit()\n",
    "print(lm.summary2())\n",
    "\n",
    "reduced_model = \"amount + grind_size + brew_time + beans + grind_size:beans\"\n",
    "lm = statsmodels.formula.api.ols(\"taste_rating ~\" + reduced_model, data=coffee_design).fit()\n",
    "print(lm.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теперь отбираетм признаки по p-value и теперь мы знаем, наиболее важные факторы для идеального кофе.\n",
    "# Профит.\n",
    "pvalues = lm.pvalues[1:]\n",
    "reduced_model = '+'.join(pvalues.loc[pvalues < 0.05].index)\n",
    "lm = statsmodels.formula.api.ols(\"taste_rating ~\" + reduced_model, data=coffee_design).fit()\n",
    "print(lm.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Планирование химического эксперимента:\n",
    "    \n",
    "    Для наглядности в рассматриваем примере всего 2: \n",
    "\n",
    "    Reaction time (40-50 minutes)\n",
    "    Temperature (80-90 °C)\n",
    "    \n",
    "    И успех оценивается в процентах: Conversion (%)\n",
    "    \n",
    "    Оптимальный дизайн будет выбираться по критерию D-optimal https://www.itl.nist.gov/div898/handbook/pri/section5/pri521.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dexpy.optimal\n",
    "from dexpy.model import make_model, ModelOrder\n",
    "from dexpy.design import coded_to_actual\n",
    "from patsy import dmatrix\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "reaction_design = dexpy.optimal.build_optimal(2, order=ModelOrder.quadratic)\n",
    "\n",
    "column_names = ['time', 'temp']\n",
    "actual_lows = { 'time': 40, 'temp': 80 }\n",
    "actual_highs = { 'time': 50, 'temp': 90 }\n",
    "\n",
    "reaction_design.columns = column_names\n",
    "\n",
    "print(coded_to_actual(reaction_design, actual_lows, actual_highs))\n",
    "\n",
    "\n",
    "quad_model = make_model(reaction_design.columns, ModelOrder.quadratic)\n",
    "X = dmatrix(quad_model, reaction_design)\n",
    "XtX = np.dot(np.transpose(X), X)\n",
    "d = np.linalg.det(XtX)\n",
    "\n",
    "print(\"|(X'X)| for quadratic 2 factor optimal design: {}\".format(d))\n",
    "\n",
    "fg = sns.lmplot('time', 'temp', data=reaction_design, fit_reg=False)\n",
    "ax = fg.axes[0, 0]\n",
    "ax.set_xticks([-1, 0, 1])\n",
    "ax.set_xticklabels(['40 min', '45 min', '50 min'])\n",
    "ax.set_yticks([-1, 0, 1])\n",
    "ax.set_yticklabels(['80C', '85C', '90C'])\n",
    "plt.show()\n",
    "\n",
    "reaction_design = dexpy.optimal.build_optimal(2, run_count=10, order=ModelOrder.quadratic)\n",
    "reaction_design.columns = column_names\n",
    "print(coded_to_actual(reaction_design, actual_lows, actual_highs))\n",
    "\n",
    "X = dmatrix(quad_model, reaction_design)\n",
    "XtX = np.dot(np.transpose(X), X)\n",
    "d = np.linalg.det(XtX)\n",
    "\n",
    "print(\"|(X'X)| for quadratic 2 factor optimal design with 10 runs: {}\".format(d))\n",
    "\n",
    "fg = sns.lmplot('time', 'temp', data=reaction_design, fit_reg=False)\n",
    "ax = fg.axes[0, 0]\n",
    "ax.set_xticks([-1, 0, 1])\n",
    "ax.set_xticklabels(['40 min', '45 min', '50 min'])\n",
    "ax.set_yticks([-1, 0, 1])\n",
    "ax.set_yticklabels(['80C', '85C', '90C'])\n",
    "plt.show()\n",
    "\n",
    "reaction_design = dexpy.optimal.build_optimal(2, run_count=10, order=ModelOrder.quadratic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# И мы получаем оптимизарованный дизайн эксперимента. Профит.\n",
    "reaction_design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Методы заполнения для рандомизированных дизайнов эксперимента\n",
    "\n",
    "1.1. Latin Hypercube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def latin_hypercube_2d_uniform(n, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    lower_limits = np.arange(0, n)/n\n",
    "    upper_limits = np.arange(1, n+1)/n\n",
    "    \n",
    "    points = np.random.uniform(low=lower_limits, high=upper_limits, size=(2, n)).T\n",
    "    np.random.shuffle(points[:,1])\n",
    "    \n",
    "    return (points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "p = latin_hypercube_2d_uniform(n, seed=137)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[5,5])\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.scatter(p[:,0],p[:,1], c='r')\n",
    "\n",
    "for i in np.arange(0,1,1/n):\n",
    "    plt.axvline(i)\n",
    "    plt.axhline(i)\n",
    "plt.grid(which='minor', alpha=0.9, linestyle='--')\n",
    "plt.title('Latin Hypercube')   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Или же ядра можно построить в библиотеке 'pyDoE'\n",
    "https://pythonhosted.org/pyDOE/rsm.html#response-surface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['Custumize sample distribution'](https://pythonhosted.org/pyDOE/_images/lhs_custom_distribution.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install pyDOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Теперь предположим, что мы хотим, чтобы эти точки\n",
    "#  были нормально распределены со средними = [0.5, 0.5] и стандартными отклонениями = [0.4, 0.2]:\n",
    "from pyDOE import *\n",
    "design = lhs(2, samples=10)\n",
    "from scipy.stats.distributions import norm\n",
    "means = [0.5, 0.5]\n",
    "stdvs = [0.4, 0.2]\n",
    "for i in range(2):\n",
    "    design[:, i] = norm(loc=means[i], scale=stdvs[i]).ppf(design[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[5, 5])\n",
    "#plt.xlim([0, 1])\n",
    "#plt.ylim([0, 1])\n",
    "\n",
    "plt.scatter(design[:,0], design[:,1], c='r')\n",
    "\n",
    "for i in np.arange(0, 1, 1 / n):\n",
    "    plt.axvline(i)\n",
    "    plt.axhline(i)\n",
    "\n",
    "plt.grid(which='minor', alpha=0.9, linestyle='--')\n",
    "plt.title('')   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### А что, если мы не будем планировать эксперимент? \n",
    "\n",
    "В случае, если повтор эксперимента не затруднительный то нам на помощь приходят методы Монте-Карло.\n",
    "\n",
    "Посмотрим на пример: Approximation of Pi Using Hit and Miss (Monte Carlo) Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Простой пример на оценку np.pi\n",
    "N = [100, 1000, 10000, 100000, 1000000]\n",
    "\n",
    "for i in range(len(N)):\n",
    "    x = np.random.uniform(low= -1, high= 1, size = [N[i], 1])\n",
    "    y = np.random.uniform(low= -1, high= 1, size = [N[i], 1])\n",
    "\n",
    "    inside_bool = x**2 +y**2<1\n",
    "\n",
    "    approx_pi = 4 * np.sum(inside_bool)/N[i]\n",
    "    print ('Pi: {}, Аппроксимация: {} '.format(np.pi, approx_pi))\n",
    "    \n",
    "    x_in=x[inside_bool]\n",
    "    y_in=y[inside_bool]\n",
    "    \n",
    "    plt.figure(figsize=[5,5])\n",
    "    plt.scatter(x, y, s=1)\n",
    "    plt.scatter(x_in, y_in, color ='r', s=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Работа в библиотеке Surrogate Optimization Toolbox (pySOT) `pySOT`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install gpy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install \"scikit-learn >= 0.18.1\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "! pip install six http://github.com/scikit-learn-contrib/py-earth/tarball/master"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install matlab_wrapper"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install subprocess32"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install pySOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Наша задача здесь находить глобальные минимумы функции. Библиотека, которую мы используем часто применяется в для моделирования химических эскпериментов. В источниках внизу ноутбука есть лекция от автора библиотеки на эту тему.\n",
    "\n",
    "Сам функционал рассчитан на моделирование процессов высокой размерности: 10 и более. \n",
    "Мы будем считать для трехмерного варианта.\n",
    "\n",
    "Зададим функцию для дальнейшей аппроксимации (Ackley)\n",
    "https://www.sfu.ca/~ssurjano/ackley.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['Ackley function'](https://www.sfu.ca/~ssurjano/ackley.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pySOT import *\n",
    "from poap.controller import SerialController, ThreadController, BasicWorkerThread\n",
    "import numpy as np\n",
    "\n",
    "# количество запусков\n",
    "maxeval = 100\n",
    "\n",
    "# (1) Проблема\n",
    "# Есть готовая функция для Ackley, ей и воспользуемся - построим для 3D функции\n",
    "data = Ackley(dim=3) \n",
    "print(data.info)\n",
    "\n",
    "# (2) Дизайн\n",
    "# Симметричные латинские гиперкубы с 2d + 1 сэмплов\n",
    "exp_des = SymmetricLatinHypercube(dim=data.dim, npts=2*data.dim+1)\n",
    "\n",
    "# (3) Суррогатная модель\n",
    "# RBF-ядро с линейным хвостом\n",
    "surrogate = RBFInterpolant(kernel=CubicKernel, tail=LinearTail, maxp=maxeval)\n",
    "\n",
    "# (4) Adaptive sampling\n",
    "#  DYCORS\n",
    "adapt_samp = CandidateDYCORS(data=data, numcand=100*data.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the serial controller (uses only one thread)\n",
    "controller = SerialController(data.objfunction)\n",
    "\n",
    "# (5) Use the sychronous strategy without non-bound constraints\n",
    "strategy = SyncStrategyNoConstraints(\n",
    "        worker_id=0, data=data, maxeval=maxeval, nsamples=1,\n",
    "        exp_design=exp_des, response_surface=surrogate,\n",
    "        sampling_method=adapt_samp)\n",
    "controller.strategy = strategy\n",
    "\n",
    "# Run the optimization strategy\n",
    "result = controller.run()\n",
    "\n",
    "# Print the final result\n",
    "print('Best value found: {0}'.format(result.value))\n",
    "print('Best solution found: {0}'.format(\n",
    "    np.array_str(result.params[0], max_line_width=np.inf,\n",
    "                precision=5, suppress_small=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract function values from the controller\n",
    "fvals = np.array([o.value for o in controller.fevals])\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(np.arange(0,maxeval), fvals, 'bo')  # Points\n",
    "ax.plot(np.arange(0,maxeval), np.minimum.accumulate(fvals), 'r-', linewidth=4.0)  # Best value found\n",
    "plt.xlabel('Evaluations')\n",
    "plt.ylabel('Function Value')\n",
    "plt.title(data.info)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция оптимизации\n",
    "class SomeFun:\n",
    "    def __init__(self, dim=10):\n",
    "        self.xlow = -10 * np.ones(dim) # lower bounds\n",
    "        self.xup = 10 * np.ones(dim) # upper bounds\n",
    "        self.dim = dim # dimensionality\n",
    "        self.info = \"Our own \" + str(dim)+\"-dimensional function\" # info\n",
    "        self.integer = np.array([0]) # integer variables\n",
    "        self.continuous = np.arange(1, dim) # continuous variables\n",
    "\n",
    "    def objfunction(self, x):\n",
    "        return np.sum(x) * np.cos(np.sum(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pySOT import check_opt_prob\n",
    "data = SomeFun(dim=3)\n",
    "check_opt_prob(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pySOT import *\n",
    "from poap.controller import SerialController, BasicWorkerThread\n",
    "\n",
    "# Decide how many evaluations we are allowed to use\n",
    "maxeval = 100\n",
    "\n",
    "# (1) Optimization problem\n",
    "data = Ackley(dim=3)\n",
    "print(data.info)\n",
    "\n",
    "# (2) Experimental design\n",
    "# Use a symmetric Latin hypercube with 2d + 1 samples\n",
    "exp_des = SymmetricLatinHypercube(dim=data.dim, npts=2*data.dim+1)\n",
    "\n",
    "# (3) Surrogate model\n",
    "# Use a cubic RBF interpolant with a linear tail\n",
    "surrogate = RBFInterpolant(kernel=CubicKernel, tail=LinearTail, maxp=maxeval)\n",
    "\n",
    "# (4) Adaptive sampling\n",
    "# Use DYCORS with 100d candidate points\n",
    "adapt_samp = CandidateDYCORS(data=data, numcand=100*data.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оптимизируем вычисления\n",
    "controller = SerialController(data.objfunction)\n",
    "\n",
    "# (5) Use the sychronous strategy without non-bound constraints\n",
    "strategy = SyncStrategyNoConstraints(\n",
    "        worker_id=0, data=data, maxeval=maxeval, nsamples=1,\n",
    "        exp_design=exp_des, response_surface=surrogate,\n",
    "        sampling_method=adapt_samp)\n",
    "controller.strategy = strategy\n",
    "\n",
    "# Run the optimization strategy\n",
    "result = controller.run()\n",
    "\n",
    "# Print the final result\n",
    "print('Best value found: {0}'.format(result.value))\n",
    "print('Best solution found: {0}'.format(\n",
    "    np.array_str(result.params[0], max_line_width=np.inf,\n",
    "                precision=5, suppress_small=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fvals = np.array([o.value for o in controller.fevals])\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(np.arange(0,maxeval), fvals, 'bo')  # Points\n",
    "ax.plot(np.arange(0,maxeval), np.minimum.accumulate(fvals), 'r-', linewidth=4.0)  # Best value found\n",
    "plt.xlabel('Evaluations')\n",
    "plt.ylabel('Function Value')\n",
    "plt.title(data.info)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, последняя оценка со введением внутренней оптимизации у нас получилась точнее всего\n",
    "\n",
    "- Best value found: 0.007130161064833107\n",
    "- Best solution found: [ 0.00296 -0.00049 -0.00036]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3*. Визуализация семплирования на 3-х мерных функциях;\n",
    "\n",
    "Смотрим вторую часть ноутбука (Материалы школы DeepBayes) - тут построены 3D визуализации и анимации Randomized Designs по поверхностям. \n",
    "\n",
    "http://nbviewer.jupyter.org/github/siri3us/ASML/blob/master/notebooks/seminar_07_gpr_open.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Gaussian graphical models с skggm\n",
    "\n",
    "https://skggm.github.io/skggm/tour"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install skggm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Import from skggm\n",
    "import inverse_covariance as ic\n",
    "from inverse_covariance import (\n",
    "    QuicGraphLasso,\n",
    "    QuicGraphLassoCV,\n",
    "    QuicGraphLassoEBIC,\n",
    "    AdaptiveGraphLasso,\n",
    "    ModelAverage\n",
    ")\n",
    "from inverse_covariance.plot_util import trace_plot\n",
    "import networkx as nx\n",
    "options_nx = {\n",
    "    'node_size': 200,\n",
    "    'linewidths': 0,\n",
    "    'width': 0.4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 15 # размерность пространства фичей(количество рёбер графа или размерность матрицы ковариаций)\n",
    "adj_type = 'banded' # 'banded', 'cluster', 'erdos-renyi', None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulate_networks import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Матрица смежности это понятно что за зверь.\n",
    "\n",
    "```Python\n",
    "prec = make_diag_dominant(adjacency)\n",
    "prec = make_correlation(prec)\n",
    "cov = np.linalg.inv(prec)\n",
    "cov = make_correlation(cov)\n",
    "\n",
    "def make_diag_dominant(adjacency):    \n",
    "    d = np.diag(np.sum(np.abs(adjacency),axis=1)+.01)\n",
    "    adjacency += d\n",
    "    return adjacency\n",
    "    \n",
    "    \n",
    "def make_correlation(adjacency):\n",
    "    \"\"\"\n",
    "    Call only after diagonal dominance is ensured. \n",
    "    \"\"\"   \n",
    "    d = np.sqrt(np.diag(adjacency))\n",
    "    adjacency /= d\n",
    "    adjacency /= d[:, np.newaxis]\n",
    "    return adjacency\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.15 * 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.2 * 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance, precision, adjacency = new_graph(15, .15, adj_type=adj_type, \n",
    "                                             random_sign=True, seed=1)\n",
    "\n",
    "covariance2, precision2, adjacency2 = new_graph(15, .2, adj_type=adj_type, \n",
    "                                                random_sign=True, seed=1)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8),dpi=300);\n",
    "nx.draw_networkx(nx.from_numpy_matrix(adjacency), ax=ax1, **options_nx)\n",
    "nx.draw_networkx(nx.from_numpy_matrix(adjacency2), ax=ax2, **options_nx)\n",
    "plt.show()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 8));\n",
    "\n",
    "compare_population_parameters(covariance, precision, adjacency, figures=(f, (ax1, ax2, ax3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(covariance2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример проблемы оценки матрицы ковариации в высоких размерностях\n",
    "\n",
    "Рассматриваем графы двух типов: со степенями вершин $2$ и $3$.\n",
    "\n",
    "При сэмплировании большого числа объектов восстановить ковариацию(или, что тоже самое, матрицу обратную к матрице смежности(adjacency)) легко.\n",
    "\n",
    "Когда объектов мало -- сложно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate multivariate normal and check empirical covariances\n",
    "def mvn(n_samples, n_features, cov, random_state=np.random.RandomState(2)):\n",
    "    X = random_state.multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "75 * n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10 * n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prng = np.random.RandomState(2)\n",
    "X1 = mvn(75 * n_features,n_features,covariance,random_state=prng)\n",
    "X2 = mvn(10 * n_features,n_features,covariance,random_state=prng)\n",
    "X3 = mvn(75 * n_features,n_features,covariance2,random_state=prng)\n",
    "X4 = mvn(10* n_features,n_features,covariance2,random_state=prng)\n",
    "\n",
    "mask = np.zeros_like(precision, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "mask[np.where(np.eye(np.shape(precision)[0]))] = True\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "f, ([ax1, ax2, ax5, ax7], [ax3, ax4, ax6, ax8]) = plt.subplots(2, 4, figsize=(26, 20));\n",
    "cov_vmax = np.max(np.triu(covariance,1))\n",
    "sns.heatmap(np.tril(np.cov(X1,rowvar=False),-1),mask=mask,cmap=cmap, vmax=cov_vmax,\n",
    "            square=True, xticklabels=5, yticklabels=5,\n",
    "            linewidths=.5, cbar_kws={\"shrink\": .5},ax=ax1)\n",
    "ax1.set_title('Sample Covariance Matrix, n/p=75')\n",
    "sns.heatmap(np.tril(np.cov(X2,rowvar=False),-1),mask=mask,cmap=cmap, vmax=cov_vmax,\n",
    "            square=True, xticklabels=5, yticklabels=5,\n",
    "            linewidths=.5, cbar_kws={\"shrink\": .5},ax=ax2)\n",
    "ax2.set_title('Sample Covariance Matrix, n/p=10')\n",
    "sns.heatmap(np.tril(sp.linalg.pinv(np.cov(X1,rowvar=False)),-1),mask=mask,cmap=cmap, vmax=cov_vmax,\n",
    "            square=True, xticklabels=5, yticklabels=5,\n",
    "            linewidths=.5, cbar_kws={\"shrink\": .5},ax=ax3)\n",
    "ax3.set_title('Sample Precision Matrix, n/p=75')\n",
    "sns.heatmap(np.tril(sp.linalg.pinv(np.cov(X2,rowvar=False)),-1),mask=mask,cmap=cmap, vmax=cov_vmax,\n",
    "            square=True, xticklabels=5, yticklabels=5,\n",
    "            linewidths=.5, cbar_kws={\"shrink\": .5},ax=ax4)\n",
    "ax4.set_title('Sample Precision Matrix, n/p=10')\n",
    "sns.heatmap(np.tril(np.cov(X3,rowvar=False),-1),mask=mask,cmap=cmap, vmax=cov_vmax,\n",
    "            square=True, xticklabels=5, yticklabels=5,\n",
    "            linewidths=.5, cbar_kws={\"shrink\": .5},ax=ax5)\n",
    "ax5.set_title('Sample Covariance Matrix, n/p=75')\n",
    "sns.heatmap(np.tril(sp.linalg.pinv(np.cov(X3,rowvar=False)),-1),mask=mask,cmap=cmap, vmax=cov_vmax,\n",
    "            square=True, xticklabels=5, yticklabels=5,\n",
    "            linewidths=.5, cbar_kws={\"shrink\": .5},ax=ax6)\n",
    "ax6.set_title('Sample Precision Matrix, n/p=75')\n",
    "sns.heatmap(np.tril(np.cov(X4,rowvar=False),-1),mask=mask,cmap=cmap, vmax=cov_vmax,\n",
    "            square=True, xticklabels=5, yticklabels=5,\n",
    "            linewidths=.5, cbar_kws={\"shrink\": .5},ax=ax7)\n",
    "ax7.set_title('Sample Covariance Matrix, n/p=10')\n",
    "sns.heatmap(np.tril(sp.linalg.pinv(np.cov(X4,rowvar=False)),-1),mask=mask,cmap=cmap, vmax=cov_vmax,\n",
    "            square=True, xticklabels=5, yticklabels=5,\n",
    "            linewidths=.5, cbar_kws={\"shrink\": .5},ax=ax8)\n",
    "ax8.set_title('Sample Precision Matrix, n/p=10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II. Sparse Inverse Covariance via Penalized MLE\n",
    "\n",
    "`path` -- \"путь\", как мы шкалируем нашу $\\lambda$, которая отвечает за $l_1$-регуляризацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# show graphical lasso path \n",
    "covariance, precision, adjacency = new_graph(15, .2, adj_type=adj_type, random_sign=True, seed=1)    \n",
    "prng = np.random.RandomState(2)\n",
    "\n",
    "X = mvn(20 * n_features, n_features, covariance, random_state=prng)\n",
    "path = np.logspace(np.log10(0.01), np.log10(1.0), num=25, endpoint=True)[::-1]\n",
    "print(path)\n",
    "estimator = QuicGraphLasso(lam=1.0, path=path, mode='path')\n",
    "estimator.fit(X)\n",
    "\n",
    "trace_plot(estimator.precision_, estimator.path_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1,ax2,ax3,ax4) = plt.subplots(1,4,figsize=(20, 8));\n",
    "ax1.imshow(estimator.precision_[0])\n",
    "ax2.imshow(estimator.precision_[10])\n",
    "ax3.imshow(estimator.precision_[15])\n",
    "ax4.imshow(estimator.precision_[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model Selection: Cross-Validation versus EBIC¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric='log_likelihood'; # 'log_likelihood', 'frobenius', 'kl', 'quadratic'\n",
    "\n",
    "covariance, precision, adjacency = new_graph(n_features, .05, adj_type='banded', random_sign=True, seed=1)\n",
    "\n",
    "f, (ax1) = plt.subplots(1, 1, figsize=(8, 6),dpi=120);\n",
    "nx.draw_networkx(nx.from_numpy_matrix(adjacency), ax=ax1, **options_nx)\n",
    "plt.show()\n",
    "\n",
    "prng = np.random.RandomState(2)\n",
    "X = mvn(10*n_features,n_features,covariance,random_state=prng)\n",
    "\n",
    "print ('QuicGraphLassoCV with:')\n",
    "print ('   metric: {}'.format(metric))\n",
    "cv_model = QuicGraphLassoCV(\n",
    "        cv=2, \n",
    "        n_refinements=6,\n",
    "        n_jobs=1,\n",
    "        init_method='cov',\n",
    "        score_metric=metric)\n",
    "cv_model.fit(X)\n",
    "cv_precision_ = cv_model.precision_\n",
    "print ('   len(cv_lams): {}'.format(len(cv_model.cv_lams_)))\n",
    "print ('   lam_scale_: {}'.format(cv_model.lam_scale_))\n",
    "print ('   lam_: {}'.format(cv_model.lam_))\n",
    "\n",
    "# EBIC\n",
    "gamma = .1 # gamma = 0 equivalent to BIC and gamma=.5 for ultra high dimensions\n",
    "ebic_model = QuicGraphLassoEBIC(\n",
    "    lam=1.0,\n",
    "    init_method='cov',\n",
    "    gamma = gamma)\n",
    "ebic_model.fit(X)\n",
    "ebic_precision_ = ebic_model.precision_\n",
    "print ('QuicGraphLassoEBIC with:')\n",
    "print ('   len(path lams): {}'.format(len(ebic_model.path_)))\n",
    "print ('   lam_scale_: {}'.format(ebic_model.lam_scale_))\n",
    "print ('   lam_: {}'.format(ebic_model.lam_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1,ax2,ax3) = plt.subplots(1,3,figsize=(20, 8));\n",
    "\n",
    "mask = np.zeros_like(precision, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "mask[np.where(np.eye(np.shape(precision)[0]))] = True\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "prec_vmax = np.max(np.triu(np.abs(adjacency),1))\n",
    "sns.heatmap(np.abs(adjacency),cmap=cmap, vmax=prec_vmax,\n",
    "            square=True, xticklabels=5, yticklabels=5,\n",
    "            linewidths=.5, cbar_kws={\"shrink\": .5},ax=ax1)\n",
    "ax1.set_title('True Precision, CV')\n",
    "\n",
    "prec_vmax = np.max(np.triu(np.abs(cv_precision_),1))\n",
    "sns.heatmap(np.abs(cv_precision_),cmap=cmap, vmax=prec_vmax,\n",
    "            square=True, xticklabels=5, yticklabels=5,\n",
    "            linewidths=.5, cbar_kws={\"shrink\": .5},ax=ax2)\n",
    "ax2.set_title('Selected Precision, CV')\n",
    "\n",
    "prec_vmax = np.max(np.triu(np.abs(ebic_precision_),1))\n",
    "sns.heatmap(np.abs(ebic_precision_),cmap=cmap, vmax=prec_vmax,\n",
    "            square=True, xticklabels=5, yticklabels=5,\n",
    "            linewidths=.5, cbar_kws={\"shrink\": .5},ax=ax3)\n",
    "ax3.set_title('Selected Precision, EBIC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part III. Compare Estimators: Initial vs. Adaptive\n",
    "\n",
    "\n",
    "#### Initial vs. Adaptive, High Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "covariance, precision, adjacency = new_graph(15, .15, adj_type=adj_type, random_sign=True, seed=1)\n",
    "f, (ax1) = plt.subplots(1, 1, figsize=(8, 6),dpi=120);\n",
    "nx.draw_networkx(nx.from_numpy_matrix(adjacency), ax=ax1, **options_nx)\n",
    "plt.show()\n",
    "\n",
    "f, (ax1,ax2,ax3) = plt.subplots(1, 3, figsize=(20, 8));\n",
    "\n",
    "n_samples = 75*n_features\n",
    "prng = np.random.RandomState(2)\n",
    "X = mvn(n_samples,n_features,covariance,random_state=prng)\n",
    "\n",
    "print ('n = {}, p = {}'.format(n_samples,n_features))\n",
    "\n",
    "compare_init_adaptive(X,n_samples,n_features,covariance=covariance, \n",
    "                      precision=precision, adjacency=adjacency, figures=(f, (ax1,ax2,ax3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial vs. Adaptive, Low Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1,ax2,ax3) = plt.subplots(1, 3, figsize=(20, 8));\n",
    "\n",
    "n_samples = 15*n_features\n",
    "prng = np.random.RandomState(2)\n",
    "X = mvn(n_samples,n_features,covariance,random_state=prng)\n",
    "print ('n = {},p = {}'.format(n_samples, n_features))\n",
    "\n",
    "compare_init_adaptive(X,n_samples,n_features,covariance=covariance, \n",
    "                      precision=precision, adjacency=adjacency, figures=(f, (ax1,ax2,ax3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial vs. Adaptive in Moderately Dense Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "covariance, precision, adjacency = new_graph(15,.4,adj_type=adj_type,random_sign=True,seed=1) \n",
    "f, (ax1) = plt.subplots(1, 1, figsize=(8, 6),dpi=120);\n",
    "nx.draw_networkx(nx.from_numpy_matrix(adjacency), ax=ax1, **options_nx)\n",
    "plt.show()\n",
    "\n",
    "f, (ax1,ax2,ax3) = plt.subplots(1, 3, figsize=(20, 8));\n",
    "compare_init_adaptive(X,n_samples,n_features,covariance=covariance, \n",
    "                      precision=precision, adjacency=adjacency, figures=(f, (ax1,ax2,ax3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 8));\n",
    "\n",
    "n_samples = 75*n_features\n",
    "prng = np.random.RandomState(2)\n",
    "X = mvn(n_samples,n_features,covariance,random_state=prng)\n",
    "print ('n = {},p = {}'.format(n_samples,n_features))\n",
    "\n",
    "compare_init_adaptive(X,n_samples,n_features,covariance=covariance, \n",
    "                      precision=precision, adjacency=adjacency, figures=(f, (ax1,ax2,ax3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 8));\n",
    "\n",
    "n_samples = 20*n_features\n",
    "prng = np.random.RandomState(2)\n",
    "X = mvn(n_samples,n_features,covariance,random_state=prng)\n",
    "print ('n = {},p = {}'.format(n_samples,n_features))\n",
    "\n",
    "compare_init_adaptive(X,n_samples,n_features,covariance=covariance, \n",
    "                      precision=precision, adjacency=adjacency, figures=(f, (ax1,ax2,ax3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV Model Averaging\n",
    "\n",
    "Опять бутстрап :)\n",
    "\n",
    "Основная идея усреднения: давайте бутстрапить исходную выборку и варьировать случайно часть гиперпараметров алгоритма(i.e. $\\lambda$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance, precision, adjacency = new_graph(15, .15, adj_type=adj_type, random_sign=True, seed=1)\n",
    "f, (ax1) = plt.subplots(1, 1, figsize=(8, 6),dpi=120);\n",
    "nx.draw_networkx(nx.from_numpy_matrix(adjacency), ax=ax1, **options_nx)\n",
    "plt.show()\n",
    "n_samples = 15 * n_features\n",
    "prng = np.random.RandomState(2)\n",
    "X = mvn(n_samples,n_features,covariance,random_state=prng)\n",
    "\n",
    "ensemble_estimator = ModelAverage(\n",
    "            n_trials=50,\n",
    "            penalization='fully-random',\n",
    "            lam=0.15)\n",
    "ensemble_estimator.fit(X)\n",
    "\n",
    "covariance, precision, adjacency = new_graph(15, .3, adj_type=adj_type, random_sign=True, seed=1)   \n",
    "f, (ax1) = plt.subplots(1, 1, figsize=(8, 6),dpi=120);\n",
    "nx.draw_networkx(nx.from_numpy_matrix(adjacency), ax=ax1, **options_nx)\n",
    "plt.show()\n",
    "n_samples = 15*n_features\n",
    "prng = np.random.RandomState(2)\n",
    "X = mvn(n_samples,n_features,covariance,random_state=prng)\n",
    "\n",
    "ensemble_estimator2 = ModelAverage(\n",
    "            n_trials=50,\n",
    "            penalization='fully-random',\n",
    "            lam=0.15)\n",
    "ensemble_estimator2.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulate_networks import _false_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "f, (ax2, ax3) = plt.subplots(1,2, figsize=(16, 8));\n",
    "\n",
    "stability_threshold = .5\n",
    "prec_hat = ensemble_estimator.proportion_\n",
    "prec_vmax = np.max(np.triu(prec_hat,1))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(prec_hat,cmap=cmap, vmax=prec_vmax,\n",
    "            square=True, xticklabels=5, yticklabels=5,\n",
    "            linewidths=.5, cbar_kws={\"shrink\": .5},ax=ax2)\n",
    "ax2.set_title('Stability Matrix, deg/p=.15')\n",
    "print ('Difference in sparsity: {},{}'.format(\n",
    "    np.sum(np.not_equal(precision,0)), \n",
    "    np.sum(np.not_equal(prec_hat>stability_threshold,0))\n",
    "))\n",
    "err_fp, err_fn = _false_support(precision,np.greater(ensemble_estimator.proportion_,stability_threshold))\n",
    "print ('Support Error:, False Pos: {}, False Neg: {}'.format(\n",
    "    err_fp,\n",
    "    err_fn\n",
    "))\n",
    " \n",
    "prec_hat = ensemble_estimator2.proportion_\n",
    "prec_vmax = np.max(np.triu(prec_hat,1))\n",
    "sns.heatmap(prec_hat,cmap=cmap, vmax=prec_vmax,\n",
    "            square=True, xticklabels=5, yticklabels=5,\n",
    "            linewidths=.5, cbar_kws={\"shrink\": .5},ax=ax3)\n",
    "ax3.set_title('Stability Matrix, deg/p=.3')\n",
    "print ('Difference in sparsity: {},{}'.format(\n",
    "    np.sum(np.not_equal(precision,0)), \n",
    "    np.sum(np.not_equal(prec_hat>stability_threshold,0))\n",
    "))\n",
    "err_fp, err_fn = _false_support(precision,np.greater(ensemble_estimator2.proportion_,stability_threshold))\n",
    "print ('Support Error:, False Pos: {}, False Neg: {}'.format(\n",
    "    err_fp,\n",
    "    err_fn\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Список полезной литературы по теме :\n",
    "\n",
    "* https://ipam.wistia.com/medias/cyg8faq730 (Видео по применению суррогатного моделирования в химии)\n",
    "* Rommel G Regis and Christine A Shoemaker. A stochastic radial basis function method for the global optimization of expensive functions. INFORMS Journal on Computing, 19(4): 497–509, 2007.\n",
    "* Rommel G Regis and Christine A Shoemaker. Parallel stochastic global optimization using radial basis functions. INFORMS Journal on Computing, 21(3):411–426, 2009.\n",
    "* Rommel G Regis and Christine A Shoemaker. Combining radial basis function surrogates and dynamic coordinate search in high-dimensional expensive black-box optimization. Engineering Optimization, 45(5): 529–555, 2013.\n",
    "* Juliane Muller and Robert Piche . Mixture surrogate models based on Dempster-Shafer theory for global optimization problems. Journal of Global Optimization, 51 (1):79–104, 2011.\n",
    "* Juliane Muller, Christine A Shoemaker, and Robert Piche SO-MI: A surrogate model algorithm for computationally expensive nonlinear mixed-integer black-box global optimization problems. Computers & Operations Research, 40(5):1383– 1400, 2013.\n",
    "* Juliane Muller, Christine A Shoemaker, and Robert Piche SO-I: a surrogate model algorithm for expensive nonlinear integer programming problems including global optimization applications. Journal of Global Optimization, 59(4):865–889, 2014."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
