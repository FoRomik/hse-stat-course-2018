{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 12. Анализ независимых компонент (ICA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## План семинара:\n",
    "\n",
    "  0. ICA (recap PCA);\n",
    "  1. Сравнение реализаций ICA;\n",
    "  2. ICA для анализа фМРТ данных;\n",
    "  3. Вспомним задачу с болезнью Альцгеймера.\n",
    "\n",
    "Источники:\n",
    "\n",
    "* https://blog.paperspace.com/dimension-reduction-with-independent-components-analysis/\n",
    "* https://towardsdatascience.com/independent-component-analysis-via-gradient-ascent-in-numpy-and-tensorflow-with-interactive-code-98b9a73e5d19\n",
    "* https://github.com/asdspal/dimRed\n",
    "* http://nilearn.github.io/auto_examples/05_advanced/plot_ica_resting_state.html\n",
    "* https://nilearn.github.io/auto_examples/03_connectivity/plot_canica_resting_state.html#sphx-glr-auto-examples-03-connectivity-plot-canica-resting-state-py\n",
    "\n",
    "Литература:\n",
    "\n",
    "* Эз а маст: https://arxiv.org/abs/1711.10873\n",
    "* https://www.sciencedirect.com/science/article/pii/S0893608000000265"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1. ICA\n",
    "\n",
    "ICA представляет собой метод линейного уменьшения размеров, который пытается восстановить независимые компоненты в сигнале. Так же ICA называют методом слепого разложения сигнала для «задачи коктейльной вечеринки». \n",
    "\n",
    "\n",
    "ICA является важным инструментом в анализе нейровизуализации, fMRI и EEG, который помогает отделить полезные сигналы от аномальных (шумов и артефактов). \n",
    "\n",
    "ICA предполагает, что каждый образец данных представляет собой смесь независимых компонент, и он стремится найти эти независимые компоненты. В основе ICA лежит «НЕЗАВИСИМОСТЬ». Мы должны сначала попытаться понять это.\n",
    "\n",
    "Когда можно смело сказать, что две переменные независимы? Чем он отличается от «Корреляции»? И, наконец, как вы измеряете степень независимости?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Пример: какой уровень корреляции между этими двумя сигналами?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "x = np.array([-5,-4,-3,-2,-1,0,1,2,3,4,5])\n",
    "y = np.array([25,16,9,4,1,0,1,4,9,16,25])\n",
    "plt.plot(x)\n",
    "plt.plot(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.correlate(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Вопрос 1: почему Гауссовые переменные плохо разделяются с ICA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Согласно Центральной предельной теореме https://en.wikipedia.org/wiki/Central_limit_theorem, сумма независимых случайных переменных ближе к гауссу чем каждая из величин сама по себе. \n",
    "\n",
    "Поэтому возникает интуиция, что чтобы восстановить исходные переменные, мы должны заложить требования «негауссовости» в них. \n",
    "\n",
    "В случае распределения Гаусса, независимые гауссовские переменные также независимы, это уникальное свойство, связанное с распределением Гаусса.\n",
    "\n",
    "Давайте рассмотрим простой пример, чтобы понять эту концепцию. Посмотрим на четыре набора данных - два из гауссовского распределения и два из равномерного распределения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Подход ICA к проблеме выделения независимых компонент основан на трех предположениях:\n",
    "\n",
    "* процесс смешивания является линейным;\n",
    "* все сигналы источника не зависят друг от друга;\n",
    "* все исходные сигналы имеют не-гауссовское распределение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Вопросы:\n",
    "\n",
    "  * Сработает ли метод с одной гауссовой переменной(или 2)?\n",
    "  * Какие переменные называются независимыми?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Почему важно что исходные сигналы имеют не-гауссовское распределение?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "U1 = np.random.uniform(-1, 1, 1000) # генерируем семплы для двумерного равномерного распределения\n",
    "U2 = np.random.uniform(-1, 1, 1000)\n",
    "\n",
    "G1 = np.random.randn(1000) # генерируем семплы для двумерного нормального распределения\n",
    "G2 = np.random.randn(1000)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(121, aspect = \"equal\")\n",
    "ax1.scatter(U1, U2, marker = \".\")\n",
    "ax1.set_title(\"Uniform\")\n",
    "\n",
    "\n",
    "ax2 = fig.add_subplot(122, aspect = \"equal\")\n",
    "ax2.scatter(G1, G2, marker = \".\")\n",
    "ax2.set_title(\"Gaussian\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# возьмем случайную Матрицу А, но которую мы домножим наши распределения\n",
    "A = np.array([[1, 0], [1, 2]])\n",
    "A = np.array([[1 / np.sqrt(2), 1 / np.sqrt(2)], [1 / np.sqrt(2), - 1 / np.sqrt(2)]])\n",
    "\n",
    "U_source = np.array([U1,U2])\n",
    "U_mix = U_source.T.dot(A)\n",
    "\n",
    "G_source = np.array([G1, G2])\n",
    "G_mix = G_source.T.dot(A)\n",
    "\n",
    "\n",
    "fig  = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.set_title(\"Mixed Uniform \")\n",
    "ax1.scatter(U_mix[:, 0], U_mix[:,1], marker = \".\")\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.set_title(\"Mixed Gaussian \")\n",
    "ax2.scatter(G_mix[:, 0], G_mix[:, 1], marker = \".\")\n",
    "\n",
    "\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA с отбелением данных (whitening)\n",
    "from sklearn.decomposition import PCA \n",
    "U_pca = PCA(whiten=True).fit_transform(U_mix)\n",
    "G_pca = PCA(whiten=True).fit_transform(G_mix)\n",
    "\n",
    "# let's plot the uncorrelated columns from the datasets\n",
    "fig  = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.set_title(\"PCA Uniform \")\n",
    "ax1.scatter(U_pca[:, 0], U_pca[:,1], marker = \".\")\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.set_title(\"PCA Gaussian \")\n",
    "ax2.scatter(G_pca[:, 0], G_pca[:, 1], marker = \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видите, в случае гауссовых переменных мы потеряли информацию о матрице A!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Вопрос 2: Как можно оценить меру негауссовости процесса?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Через kurtosis(коэффициент эксцесса) и негэнтропию: http://fourier.eng.hmc.edu/e161/lectures/ica/node4.html\n",
    "\n",
    "Идея в том, что гаусс обладает самой высокой энтропией среди всех распределений с одинаковой дисперсией, поэтому если мы хотим найти негауссово распределение, то нам нужно добавить регуляризацию на его энтропию:\n",
    "\n",
    "$$H(\\mathcal{N}(0, \\sigma^2)) - H(p(x))) \\geq C$$\n",
    "\n",
    "И интересной особенностью этого выражения является то что оно выражается через коэффициент эксцесса, т.е.\n",
    "\n",
    "$$H(\\mathcal{N}(0, \\sigma^2)) - H(p(y))) \\approx \\frac{1}{12} \\mathbb E[y^3]^2 + \\frac{1}{48} kurt(x)^3 \\geq C$$\n",
    "\n",
    "Однако так как мы рассматриваем совместное распределение большого числа величин, то удобнее вести речь в терминах энтропий и взаимных информаций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Избавляется от нелинейной зависимости\n",
    "\n",
    "Мы можем минимизировать нелинейную зависимость, уменьшая взаимную информацию между переменными. Из теории информации взаимную информацию можно определить как:\n",
    "\n",
    "<center>$MI$ = сумма энтропий маргинальных функций распределения - энтропия функции совместного распределения:<p>\n",
    "    \n",
    "Где энтропия - это показатель средней информации, содержащейся в данных. Для переменной $ x $ с распределением $p(x)$ она определяется как\n",
    "\n",
    "$$ H(x) = - \\sum\\limits_{i}\\,(\\,p\\,(x_i) \\times \\log_2 p(\\,x_i\\,))$$\n",
    "    \n",
    "![](http://fourier.eng.hmc.edu/e161/lectures/figures/mutual_info.gif)\n",
    "\n",
    "Итак, для нахождения энтропии двух переменных $ x $, $ y $ мы должны минимизировать следующую функцию, где $z = p(x, y) $ совместная плотность распределения  $x$ и $y$:\n",
    "    \n",
    "$$MI = H(x)+ H(y) - H(z)$$\n",
    "\n",
    "\n",
    "Предположим, что W - это матрица преобразования, которую нам нужно умножить на $X_{new}$, чтобы удалить зависимость более высокого порядка. Затем мы можем определить\n",
    "\n",
    "$$MI = \\sum\\limits_{i}H(W_i\\times X_{pca_{i}}) - H(W\\times X_{pca})$$\n",
    "    \n",
    "$$MI = \\sum\\limits_{i}H(W_i\\times X_{pca_{i}}) - H(X_{pca}) + \\log_2 |W|$$\n",
    "\n",
    "\n",
    "Поскольку $ W $ является матрицей вращения, ее определитель равен 1, поэтому $ \\log_2 |W| = 0 $. Теперь $ H (X_ {pca}) $ не зависит от $ W $, поэтому мы можем отбросить его:\n",
    "\n",
    "\n",
    "$$MI =  \\sum\\limits_{i}(H(W_i\\times X_{pca_{i}}))$$\n",
    "\n",
    "\n",
    "Таким образом,  \n",
    "\n",
    "\n",
    "$$W = \\arg\\min_{W}\\sum\\limits_{i}(H(W_i\\times X_{pca_{i}}))$$\n",
    "    \n",
    "\n",
    "Есть еще один способ найти матрицу $W_{negentropy} $, которая определяется выражением \n",
    "\n",
    "$$J(y) = H(y_g) - H(y)$$\n",
    "    \n",
    "\n",
    "Где $ H $ - энтропия, $ y $ - наш $X_ {pca}$, а $ x_ {gauss} $ - гауссовский случайный вектор с той же ковариацией, что и $ y $.\n",
    "      \n",
    "Среди всех распределений распределение Гауссово, как известно имеет наибольшую энтропию. Негентропия равна нулю, если $ y $ гауссова, и ненулевая, когда $ y $ не гауссов. Мы можем максимизировать функцию негэнтропии, чтобы получить матрицу $ W $.\n",
    "\n",
    "При этом, $MI = константа - J (y)$.\n",
    "    \n",
    "Таким образом, оба метода сводятся к расчету энтропии, для которого нам нужно знать плотность распределения неизвестных переменных.\n",
    "\n",
    "Оценивать распределения задача неблагодарная, плюс численно сложная, поэтому нужно делать аппроксимации.\n",
    "\n",
    "Как мы говорили выше, одна из возможных аппроксимаций выражается через куртозис и третий момент, но такое приближение не очень робастное.\n",
    "\n",
    "__(Hyvärinen, 1998b)__ предложил более хороший способ оценки негэнтропии:\n",
    "\n",
    "$$J(y) = \\sum\\limits_{i=1}^{p} k_i \\left[ \\mathbb E G_i(y) - \\mathbb E G_i(\\mathcal N(0, 1))\\right]^2, $$\n",
    "\n",
    "*1. Заметим, что если $G = y^4$, то это превращается в формулу с куртозисом, т.е. это обобщение более частного уравнения.*\n",
    "\n",
    "*2. Заметим, что второй член не зависит от данных, поэтому далее мы его опустим*\n",
    "\n",
    "Где $g_i$ — некоторые хорошие функции. Примеры хороших функций:\n",
    "\n",
    "$$G_1(y) =  \\tanh(ay)$$\n",
    "\n",
    "$$G_2(y) = -\\exp(-y^2/2)$$\n",
    "\n",
    "$$G_3(y) = -y \\exp(-y^2/2)$$\n",
    "\n",
    "$$G_4(y) = \\log \\cosh a_1 y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Псевдокод ICA\n",
    "\n",
    "http://fourier.eng.hmc.edu/e161/lectures/ica/node7.html\n",
    "\n",
    "Напомню, что мы ищем матрицу $W$, для следующего линейного преобразования данных $X$ в декоррелированные данные $Y$: \n",
    "\n",
    "$$Y = W X$$\n",
    "\n",
    "Выписываем лагранжиан с использованием теоремы Каруша-Куна-Таккера(для обеспечения нормировки векторов $w$):\n",
    "\n",
    "$$\\mathcal{L} = \\mathbb{E} G(w^T x) - \\beta(w^T w - 1) / 2$$\n",
    "\n",
    "Теперь мы можем свести задачу к задаче оптимизации функционала методом Ньютона(градиентный метод второго порядка). Тогда после нехитрых шагов выписывания производных, мы получаем красивую формулу для обновления текущей оценки W:\n",
    "\n",
    "\n",
    "$$W = \\mathbb E\\{ XG(W^T X) \\} - \\mathbb E\\{g(W^T X)\\}W$$\n",
    "\n",
    "$$W = \\frac{W}{||W||}$$\n",
    " \n",
    "где $G$ - аппроксимирующая функция, а $g$ - производная от $G$.\n",
    "\n",
    "Прочитать про имплементацию можно [здесь](https://www.cs.helsinki.fi/u/ahyvarin/papers/bookfinal_ICA.pdf).\n",
    "\n",
    "Псевдокод для нахождения W матриц:\n",
    "\n",
    "1.$\\hspace{1em}$ for $i$ in (1, number of sources):\n",
    "2.    $\\hspace{1em}W_i$ = random vector (initialization of  $W_i$) <p>\n",
    "    $\\hspace{1em}$while $W$ changes: \n",
    "4. $\\hspace{2em}W_i$ = $\\mathbb E\\{XG($$W_i^T$$ X)\\} - \\mathbb E\\{g(W_i^T X\\}W_i\\hspace{2em}$ (update of w)\n",
    "5.    $\\hspace{2em}W_i$ = $W_i$ - $\\sum\\limits_{j=0}^{i-1}W_iW_jW_j$   $\\hspace{1em}$(orthogonalization of $W_i$ w.r.t other vectors of $W$ using gram-schmidt process)\n",
    "6.    $\\hspace{2em}W = W/||W||$\n",
    " \n",
    "Таким образом, мы получаем $W_i$ ортогональную всем  $W$ векторам.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approximating function is G(x) = x \\exp(-x**2 / 2)\n",
    "def g(x):\n",
    "    return <YOUR cosine>\n",
    "\n",
    "def gprime(x):\n",
    "    return <YOUR cosine derivative>\n",
    "\n",
    "\n",
    "# update of Wi\n",
    "def update_vector(W, X):\n",
    "    Y1 = W.dot(X)\n",
    "    f1 =  (X * g(Y1)).mean(axis = 1)\n",
    "    f2 = gprime(Y1).mean() * W\n",
    "    W = f1 - f2\n",
    "    return(W)\n",
    "\n",
    "# orthonormalize the Wi\n",
    "def orthonormalize(W, w, k):\n",
    "    C= np.zeros_like(w)\n",
    "    \n",
    "    for j in range(k):\n",
    "        C = C + np.dot(w, W[j].T) * W[j]\n",
    "        \n",
    "    w = w - C\n",
    "    return(w / np.sqrt((w * w).sum()))\n",
    "\n",
    "# calculation of unmixing matrix\n",
    "# X.shape = number_of_dimensions, number of samples\n",
    "def calculate_V(X, W_init):\n",
    "    n_f, n_sample = X.shape\n",
    "    W = np.zeros((n_f, n_f))\n",
    "    \n",
    "    for i in range(n_f):\n",
    "        w = W_init[i, :].copy()\n",
    "        w /= np.sqrt((w**2).sum())\n",
    "        w_old = np.zeros((1, n_f))\n",
    "        j=0\n",
    "        max_iter = 1000\n",
    "        while (np.abs(np.abs((w*w_old).sum())-1) > 1e-3 and (j < max_iter-1)):\n",
    "            w_old = w\n",
    "            w = update_vector(w, X)\n",
    "            w = orthonormalize(W, w, i)\n",
    "            j +=1\n",
    "        W[i, :] = w\n",
    "    return(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection of number of components\n",
    "# W = unmixing matrix\n",
    "# S = source signal matrix\n",
    "def ICA(X, n_comp=None):\n",
    "    \n",
    "    X = (X - X.mean(axis = 0))/(X.std(axis=0))\n",
    "    pca = PCA(whiten=True)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    explained_var = pca.explained_variance_\n",
    "    if not n_comp:\n",
    "        n_comp = np.argmax(np.cumsum(explained_var)/explained_var.sum() > 0.99)+1\n",
    "    K = pca.components_[0:n_comp]/pca.singular_values_\n",
    "    X_pca = X_pca[:,0:n_comp]\n",
    "    W_init = np.random.rand(n_comp, n_comp)\n",
    "    V = calculate_V(X_pca.T, W_init)\n",
    "    S = V.dot(X_pca.T)\n",
    "    \n",
    "    W = V.dot(K)\n",
    "    \n",
    "    return(W, S,V,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "np.random.seed(42)\n",
    "num_rows = 3000\n",
    "n_samples=1500\n",
    "t = np.linspace(0,10, n_samples)\n",
    "# create signals sources\n",
    "s1 = np.sin(3*t) # a sine wave\n",
    "s2 = np.sign(np.cos(6*t)) # a square wave\n",
    "s3 = signal.sawtooth(2 *t) # a sawtooth wave\n",
    "# combine single sources to create a numpy matrix\n",
    "S = np.c_[s1,s2,s3]\n",
    "\n",
    "# add a bit of random noise to each value\n",
    "S += 0.2 * np.random.normal(size = S.shape)\n",
    "\n",
    "# create a mixing matrix A\n",
    "A = np.array([[1, 1.5, 0.5], [2.5, 1.0, 2.0], [1.0, 0.5, 4.0]])\n",
    "X = S.dot(A.T)\n",
    "\n",
    "#plot the single sources and mixed signals\n",
    "plt.figure(figsize =(18, 8))\n",
    "colors = ['red', 'blue', 'orange']\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.title('True Sources')\n",
    "for color, series in zip(colors, S.T):\n",
    "    plt.plot(series, color)\n",
    "plt.subplot(2,1,2)\n",
    "plt.title('Observations(mixed signal)')\n",
    "for color, series in zip(colors, X.T):\n",
    "    plt.plot(series, color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теперь разложим на независимые компоненты наш сигнал:\n",
    "W_, S_,V_ , K_= ICA(X, 3)\n",
    "plt.figure(figsize =(18, 8))\n",
    "plt.title('Recovered signals')\n",
    "for color, series in zip(colors, S_):\n",
    "    plt.plot(series, color)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Вопрос 3: Откуда мы знали, что нужно раскладывать на 3 компоненты?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "СМЕКАЛОЧКА! Мы угодали сколько и какие компоненты выбрать.\n",
    "\n",
    "ICA ничего нам не говорит о порядке компонентов или о том, сколько из них имеет значение. Таким образом, найти правильное количество компонент сложно.\n",
    "\n",
    "* Мы можем ограничить количество компонентов с помощью PCA(по уровню explained variance первых компонент), затем найти такое же количество независимых компонентов;\n",
    "* Мы можем оценить внутренную размерность сэмпла, допустим с MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of 'Maximum Likelihood Estimation of Intrinsic Dimension' by Elizaveta Levina and Peter J. Bickel\n",
    " \n",
    "how to use\n",
    "----------\n",
    " \n",
    "The goal is to estimate intrinsic dimensionality of data, \n",
    "the estimation of dimensionality is scale dependent\n",
    "(depending on how much you zoom into the data distribution\n",
    "you can find different dimesionality), so they\n",
    "propose to average it over different scales, \n",
    "the interval of the scales [k1, k2] are the only parameters of the algorithm.\n",
    " \n",
    "This code also provides a way to repeat the estimation with bootstrapping to estimate uncertainty.\n",
    " \n",
    "Here is one example with swiss roll :\n",
    " \n",
    "from sklearn.datasets import make_swiss_roll\n",
    "X, _ = make_swiss_roll(1000)\n",
    " \n",
    "k1 = 10 # start of interval(included)\n",
    "k2 = 20 # end of interval(included)\n",
    "intdim_k_repeated = repeated(intrinsic_dim_scale_interval, \n",
    "                             X, \n",
    "                             mode='bootstrap', \n",
    "                             nb_iter=500, # nb_iter for bootstrapping\n",
    "                             verbose=1, \n",
    "                             k1=k1, k2=k2)\n",
    "intdim_k_repeated = np.array(intdim_k_repeated)\n",
    "# the shape of intdim_k_repeated is (nb_iter, size_of_interval) where \n",
    "# nb_iter is number of bootstrap iterations (here 500) and size_of_interval\n",
    "# is (k2 - k1 + 1).\n",
    " \n",
    "# Plotting the histogram of intrinsic dimensionality estimations repeated over\n",
    "# nb_iter experiments\n",
    "plt.hist(intdim_k_repeated.mean(axis=1))\n",
    " \n",
    "\"\"\"\n",
    "# from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    " \n",
    "def intrinsic_dim_sample_wise(X, k=5):\n",
    "    neighb = <YOUR CODE>\n",
    "    dist, ind = neighb.kneighbors(X) # distances between the samples and points\n",
    "    dist = dist[:, 1:] # the distance between the first points to first points (as basis ) equals zero\n",
    "    # the first non trivial point\n",
    "    dist = dist[:, 0:k]# including points k-1\n",
    "    assert dist.shape == (X.shape[0], k) # requirments are there is no equal points\n",
    "    assert np.all(dist > 0)\n",
    "    d = np.log(dist[:, k - 1: k] / dist[:, 0:k-1]) # dinstanec betveen the bayeasan statistics\n",
    "    d = d.sum(axis=1) / (k - 2)\n",
    "    d = 1. / d\n",
    "    intdim_sample = d\n",
    "    return intdim_sample\n",
    " \n",
    "def intrinsic_dim_scale_interval(X, k1=10, k2=20):\n",
    "    X = pd.DataFrame(X).drop_duplicates().values # remove duplicates in case you use bootstrapping\n",
    "    intdim_k = []\n",
    "    for k in range(k1, k2 + 1): # in order to reduse the noise by eliminating of the nearest neibours \n",
    "        m = intrinsic_dim_sample_wise(X, k).mean()\n",
    "        intdim_k.append(m)\n",
    "    return intdim_k\n",
    " \n",
    "def repeated(func, X, nb_iter=100, random_state=None, mode='bootstrap', **func_kw):\n",
    "    if random_state is None:\n",
    "        rng = np.random\n",
    "    else:\n",
    "        rng = np.random.RandomState(random_state)\n",
    "    nb_examples = X.shape[0]\n",
    "    results = []\n",
    " \n",
    "    iters = range(nb_iter) \n",
    "    for i in iters:\n",
    "        if mode == 'bootstrap':# and each point we want to resample with repeating points to reduse the errors \n",
    "            #232 111 133 \n",
    "            Xr = X[rng.randint(0, nb_examples, size=nb_examples)]\n",
    "        elif mode == 'shuffle':\n",
    "            ind = np.arange(nb_examples)\n",
    "            rng.shuffle(ind)\n",
    "            Xr = X[ind]\n",
    "        elif mode == 'same':\n",
    "            Xr = X\n",
    "        else:\n",
    "            raise ValueError('unknown mode : {}'.format(mode))\n",
    "        results.append(func(Xr, **func_kw))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = 2 # start of interval(included)\n",
    "k2 = 20 # end of interval(included)\n",
    "nb_iter = 20 # more iterations more accuracy\n",
    "# intrinsic_dim_scale_interval gives better estimation\n",
    "intdim_k_repeated = repeated(intrinsic_dim_scale_interval, \n",
    "                             X, \n",
    "                             mode='bootstrap', \n",
    "                             nb_iter=nb_iter, # nb_iter for bootstrapping\n",
    "                             k1=k1, k2=k2)\n",
    "intdim_k_repeated = np.array(intdim_k_repeated)\n",
    "\n",
    "\n",
    "print (np.shape(intdim_k_repeated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(k1, k2+1)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(x, np.mean(intdim_k_repeated, axis=0), 'b', label='Mean') # it is the mean walue\n",
    "plt.fill_between(x, \n",
    "                 np.mean(intdim_k_repeated, axis=0) - \n",
    "                 2 * np.std(intdim_k_repeated, axis=0),\n",
    "                 np.mean(intdim_k_repeated, axis=0) + \n",
    "                 2 * np.std(intdim_k_repeated, axis=0),\n",
    "                 alpha=0.3,\n",
    "                 label='CI=95%',\n",
    "                 color='g')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Сравнение реализаций ICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кратко про метод: \n",
    "\n",
    "https://drive.google.com/viewerng/viewer?url=http://cs229.stanford.edu/notes/cs229-notes11.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, FastICA\n",
    "from numpy import dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерируем сэмпл\n",
    "rng = np.random.RandomState(42)\n",
    "S = rng.standard_t(1.5, size=(20000, 2))\n",
    "S[:, 0] *= 2.\n",
    "\n",
    "# Mix data\n",
    "A = np.array([[1, 1], [0, 2]])  # Mixing matrix\n",
    "\n",
    "X = np.dot(S, A.T)  # Generate observations\n",
    "X_mean = X.mean()\n",
    "X = X - X_mean #shift mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca = PCA()\n",
    "S_pca_ = pca.fit(X).transform(X)\n",
    "S_pca_ /= S_pca_.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICA\n",
    "ica = FastICA(random_state=rng)\n",
    "S_ica_ = ica.fit(X).transform(X)  # Estimate the sources\n",
    "S_ica_ /= S_ica_.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_sig(x):return  1 /(1 + (np.exp(-x)))\n",
    "def np_tanh(x): return np.tanh(x)\n",
    "\n",
    "num_epoch = 10000\n",
    "learning_rate = 0.00003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICA со спуском по сигмоиде\n",
    "# Читать здесь http://www.sci.utah.edu/~shireen/pdfs/tutorials/Elhabian_ICA09.pdf\n",
    "w_sig = np.eye(2)\n",
    "for iter in range(num_epoch):\n",
    "    temp = np_sig(dot(X, w_sig))\n",
    "    temp = 1-2*temp\n",
    "    w_sig = w_sig + learning_rate * (np.linalg.inv(w_sig.T) + dot(temp.T, X))\n",
    "\n",
    "S_sig_grad = dot(X,w_sig)\n",
    "S_sig_grad /= S_sig_grad.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICA со спуском по tanh\n",
    "# Elhabian method\n",
    "# Читать здесь http://www.sci.utah.edu/~shireen/pdfs/tutorials/Elhabian_ICA09.pdf\n",
    "w_tanh_grad = np.eye(2)\n",
    "for current_iter in range(num_epoch):\n",
    "    u = dot(X,w_tanh_grad)\n",
    "    U = np_tanh(u)\n",
    "    g = np.linalg.inv(w_tanh_grad.T) - (2 / len(X)) * dot(X.T, U)\n",
    "    w_tanh_grad = w_tanh_grad + learning_rate * g\n",
    "\n",
    "S_tanh_grad = dot(X,w_tanh_grad)\n",
    "S_tanh_grad /= S_tanh_grad.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original Data Shape',S.shape)\n",
    "print('Fast ICA Data Shape',S_ica_.shape)\n",
    "print('PCA Data Shape',S_pca_.shape)\n",
    "print('Sigmoid Grad Data Shape',S_sig_grad.shape)\n",
    "print('Tanh Grad Data Shape',S_tanh_grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(S, axis_list=None):\n",
    "    plt.scatter(S[:, 0], S[:, 1], s=2, marker='o', zorder=10,\n",
    "                color='steelblue', alpha=0.5)\n",
    "    if axis_list is not None:\n",
    "        colors = ['orange', 'red']\n",
    "        for color, axis in zip(colors, axis_list):\n",
    "            axis /= axis.std()\n",
    "            x_axis, y_axis = axis\n",
    "            # Trick to get legend to work\n",
    "            plt.plot(0.1 * x_axis, 0.1 * y_axis, linewidth=2, color=color)\n",
    "            plt.quiver(0, 0, x_axis, y_axis, zorder=11, width=0.01, scale=6,\n",
    "                       color=color)\n",
    "\n",
    "    plt.hlines(0, -3, 3)\n",
    "    plt.vlines(0, -3, 3)\n",
    "    plt.xlim(-3, 3)\n",
    "    plt.ylim(-3, 3)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(3, 2, 1)\n",
    "plot_samples(S / S.std())\n",
    "plt.title('True Independent Sources')\n",
    "\n",
    "axis_list = [pca.components_.T, ica.mixing_]\n",
    "plt.subplot(3, 2, 2)\n",
    "plot_samples(X / np.std(X), axis_list=axis_list)\n",
    "legend = plt.legend(['PCA', 'ICA'], loc='upper right')\n",
    "legend.set_zorder(100)\n",
    "\n",
    "plt.title('Observations')\n",
    "\n",
    "plt.subplot(3, 2, 3)\n",
    "plot_samples(S_pca_ / np.std(S_pca_, axis=0))\n",
    "plt.title('PCA recovered signals')\n",
    "\n",
    "plt.subplot(3, 2, 4)\n",
    "plot_samples(S_ica_ / np.std(S_ica_))\n",
    "plt.title('ICA recovered signals')\n",
    "\n",
    "plt.subplot(3, 2, 5)\n",
    "plot_samples(S_sig_grad / np.std(S_sig_grad, axis=0))\n",
    "plt.title('ICA sig recovered signals')\n",
    "\n",
    "plt.subplot(3, 2, 6)\n",
    "plot_samples(S_tanh_grad / np.std(S_tanh_grad))\n",
    "plt.title('ICA tanh recovered signals')\n",
    "\n",
    "plt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.36)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastICA и ICA с функцией tanh лучше всего справились с разложением.\n",
    "\n",
    "Заметим, что по умолчанию sklearn `FastICA` использует $\\log \\cosh(y)$ функцию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ICA для анализа fMRT данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "# Here we use only 3 subjects to get faster-running code. For better\n",
    "# results, simply increase this number\n",
    "# XXX: must get the code to run for more than 1 subject\n",
    "dataset = datasets.fetch_adhd(n_subjects=1)\n",
    "func_filename = dataset.func[0]\n",
    "\n",
    "# print basic information on the dataset\n",
    "print('First subject functional nifti image (4D) is at: %s' %\n",
    "      dataset.func[0])  # 4D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.input_data import NiftiMasker\n",
    "\n",
    "# This is resting-state data: the background has not been removed yet,\n",
    "# thus we need to use mask_strategy='epi' to compute the mask from the\n",
    "# EPI images\n",
    "masker = NiftiMasker(smoothing_fwhm=8, memory='nilearn_cache', memory_level=1,\n",
    "                     mask_strategy='epi', standardize=True)\n",
    "data_masked = masker.fit_transform(func_filename)\n",
    "\n",
    "# Concatenate all the subjects\n",
    "# fmri_data = np.concatenate(data_masked, axis=1)\n",
    "fmri_data = data_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 20\n",
    "ica = FastICA(n_components=n_components, random_state=42)\n",
    "components_masked = ica.fit_transform(data_masked.T).T\n",
    "\n",
    "# Normalize estimated components, for thresholding to make sense\n",
    "components_masked -= components_masked.mean(axis=0)\n",
    "components_masked /= components_masked.std(axis=0)\n",
    "# Threshold\n",
    "import numpy as np\n",
    "components_masked[np.abs(components_masked) < .8] = 0\n",
    "\n",
    "# Now invert the masking operation, going back to a full 3D\n",
    "# representation\n",
    "component_img = masker.inverse_transform(components_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some interesting components\n",
    "from nilearn import image\n",
    "from nilearn.plotting import plot_stat_map, show\n",
    "\n",
    "# Use the mean as a background\n",
    "mean_img = image.mean_img(func_filename)\n",
    "\n",
    "plot_stat_map(image.index_img(component_img, 0), mean_img)\n",
    "\n",
    "plot_stat_map(image.index_img(component_img, 2), mean_img)\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы выбрали 10 первых компонент и построили 2 из них. А какая размерность данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = 1 # start of interval(included)\n",
    "k2 = 5 # end of interval(included)\n",
    "nb_iter = 2 # more iterations more accuracy\n",
    "# intrinsic_dim_scale_interval gives better estimation\n",
    "intdim_k_repeated = repeated(intrinsic_dim_scale_interval, \n",
    "                             data_masked, \n",
    "                             mode='bootstrap', \n",
    "                             nb_iter=nb_iter, # nb_iter for bootstrapping\n",
    "                             k1=k1, k2=k2)\n",
    "intdim_k_repeated = np.array(intdim_k_repeated)\n",
    "\n",
    "\n",
    "print (np.shape(intdim_k_repeated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(k1, k2+1)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(x, np.mean(intdim_k_repeated, axis=0), 'b', label='Mean') # it is the mean walue\n",
    "plt.fill_between(x, \n",
    "                 np.mean(intdim_k_repeated, axis=0) - \n",
    "                 2 * np.std(intdim_k_repeated, axis=0),\n",
    "                 np.mean(intdim_k_repeated, axis=0) + \n",
    "                 2 * np.std(intdim_k_repeated, axis=0),\n",
    "                 alpha=0.3,\n",
    "                 label='CI=95%',\n",
    "                 color='g')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Вспомним задачу, которую мы не решили с PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "from matplotlib import offsetbox\n",
    "\n",
    "def plot_embedding(X, y, images_small=None, title=None):\n",
    "    \"\"\"\n",
    "    Функция для рисования эмбеддингов \n",
    "    с цветами соответствующими y\n",
    "    \"\"\"\n",
    "    # take only first two columns\n",
    "    X = X[:, :2]\n",
    "    # scaling\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "    plt.figure(figsize=(13,8))\n",
    "    ax = plt.subplot(111)\n",
    "    for i in range(X.shape[0] - 1):\n",
    "        plt.text(X[i, 0], X[i, 1], str(y[i]),\n",
    "                 color=plt.cm.RdGy(y[i]),\n",
    "                 fontdict={'weight': 'bold', 'size': 12})\n",
    "        if images_small is not None:\n",
    "            imagebox = OffsetImage(images_small[i], zoom=.3)\n",
    "            ab = AnnotationBbox(imagebox, (X[i, 0], X[i, 1]),\n",
    "                xycoords='data')                                  \n",
    "            ax.add_artist(ab)\n",
    "    if hasattr(offsetbox, 'AnnotationBbox'):\n",
    "        # only print thumbnails with matplotlib > 1.0\n",
    "        shown_images = np.array([[1., 1.]])  \n",
    "        for i in range(X.shape[0]):\n",
    "            dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
    "            if np.min(dist) < 4e-1:\n",
    "                # don't show points that are too close\n",
    "                continue\n",
    "    if title is not None:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('./data.csv')\n",
    "mask = data.CDR.notnull()\n",
    "data = data[mask]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "def spectrum(x):\n",
    "    f = np.fft.fft2(x)\n",
    "    fshift = np.fft.fftshift(f)\n",
    "    magnitude_spectrum = np.log(np.abs(fshift))\n",
    "    return magnitude_spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_data = pd.read_csv('./brain_data.csv.zip')[mask]\n",
    "brain_data.drop(labels=['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_brain = np.log(brain_data.values / 255. + 1)\n",
    "X_brain_image = X_brain.reshape((-1, 176, 176))\n",
    "age = data.Age.values\n",
    "cdr = (data.CDR.values == 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_brain_image[2], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, FastICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_projected = FastICA(n_components=3, max_iter=1000).fit_transform(brain_data)\n",
    "\n",
    "plot_embedding(X_projected, cdr, X_brain_image, \"ICA. Pis.\")\n",
    "plot_embedding(X_projected, age, title=\"ICA. CDR.\")\n",
    "plot_embedding(X_projected, cdr, title=\"ICA. CDR.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "scores = cross_val_score(estimator=XGBClassifier(), \n",
    "                         X=X_projected, \n",
    "                         y=LabelEncoder().fit_transform(cdr), \n",
    "                         cv=5,\n",
    "                         scoring='roc_auc')\n",
    "\n",
    "print('Средняя точность: {} +- {}'.format(1 - np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
